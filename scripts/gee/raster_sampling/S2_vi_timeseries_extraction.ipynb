{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089e535-a9d1-47df-82aa-028ee2a5aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "\n",
    "import geemap\n",
    "import os\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65a05f",
   "metadata": {},
   "source": [
    "# 1. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d18ee5-35d0-41e7-a4f8-9421fec1cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud filter params\n",
    "CLOUD_FILTER = 90\n",
    "CLD_PRB_THRESH = 50\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 1\n",
    "BUFFER = 50\n",
    "\n",
    "# bounding box params\n",
    "COUNTRY = ''\n",
    "STATE = 'AK' #Abbreviated for WATERSHED\n",
    "POINTBUFFER = 30 #meters\n",
    "ROI = 'HUC' #STATE, COUNTRY, BBOX, or HUC\n",
    "HUCLIST = ['190604', '190603', '190602'] #must be list\n",
    "GEOJSON_PATH = ''\n",
    "GRIDSIZE = 18000 #km*1000\n",
    "DIR_PATH = '/mnt/poseidon/remotesensing/arctic/data/rasters/S2SR'\n",
    "\n",
    "# data Information\n",
    "SCALE = 10\n",
    "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
    "start_date = date(2019, 1, 1) # Year-Month-Day (minus 5 days to make it an even 38, 30 day intervals)\n",
    "end_date = date(2019, 12, 26) # Year-Month-Day\n",
    "INCREMENT = 15 #days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d27bc",
   "metadata": {},
   "source": [
    "# 2. Create time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_of_dates(start_date, end_date):\n",
    "    dates = []\n",
    "    delta = end_date - start_date   # returns timedelta\n",
    "\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        dates.append(day)\n",
    "    return dates\n",
    "\n",
    "def create_time_intervals(dates_list, Interval):\n",
    "    time_df = pd.DataFrame({'Date': dates_list}).astype('datetime64[ns]')\n",
    "    interval = timedelta(Interval)\n",
    "    grouped_cr = time_df.groupby(pd.Grouper(key='Date', freq=interval))\n",
    "    date_ranges = []\n",
    "    for i in grouped_cr:\n",
    "        date_ranges.append(((str(i[1].min()[0]).split(' ')[0]), (str(i[1].max()[0]).split(' ')[0])))\n",
    "    return date_ranges\n",
    "\n",
    "date_ranges = create_time_intervals(create_list_of_dates(start_date, end_date), INCREMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab97fd",
   "metadata": {},
   "source": [
    "# 3. Select test region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb09f5",
   "metadata": {},
   "source": [
    "## 3.1. Get test region string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e58014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create huc (rank) directories\n",
    "CURRENTROI = HUCLIST[0] # select one huc from list to work with\n",
    "PATH = f'{DIR_PATH}/{CURRENTROI}'\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef98d2",
   "metadata": {},
   "source": [
    "## 3.2. Create test region output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08973275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp directories within each huc (rank)\n",
    "if os.path.isdir(PATH):\n",
    "    print(f'{PATH} already exists.')\n",
    "else:\n",
    "    os.mkdir(PATH)\n",
    "    print(f'Created {PATH}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6fce3",
   "metadata": {},
   "source": [
    "## 3.3. Select test region in GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import admin data and select country to create grid around\n",
    "if ROI == 'STATE':\n",
    "    grid_location_ee = (ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
    "                        .filterMetadata('ADM0_NAME', 'equals', COUNTRY)\n",
    "                        .filterMetadata('ADM1_NAME', 'equals', STATE))\n",
    "\n",
    "elif ROI == 'COUNTRY':\n",
    "    grid_location_ee = (ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
    "                        .filterMetadata('ADM0_NAME', 'equals', COUNTRY))\n",
    "\t\n",
    "elif ROI == 'BBOX':\n",
    "\tgrid_location_ee = geemap.geojson_to_ee(GEOJSON_PATH)\n",
    "    \n",
    "elif ROI == 'HUC':\n",
    "    grid_location_ee = (ee.FeatureCollection(\"USGS/WBD/2017/HUC06\")\n",
    "                        .filter(ee.Filter.inList('huc6', [CURRENTROI])))\n",
    "    \n",
    "else:\n",
    "    print('Invalid region of interest. Check STATE, COUNTRY, HUC')\n",
    "    grid_location_ee = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f88d7",
   "metadata": {},
   "source": [
    "# 4. Select observation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246db0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation points for later\n",
    "d = '/mnt/poseidon/remotesensing/arctic/data/vectors/AK-AVA_Turboveg/ak_tvexport_releves_header_data_for_vegbank_20181106_ALB.xlsx'\n",
    "obs_data = pd.read_excel(d, skiprows=[1])\n",
    "obs_data = obs_data.replace(-9, np.nan)\n",
    "\n",
    "# extract geometry and unique ID\n",
    "obs_geom = obs_data[['Latitude (decimal degrees)', 'Longitude (decimal degrees)', 'Releve number']]\n",
    "#obs_geom.set_index('Releve number', inplace=True)\n",
    "\n",
    "# create ee object (feature collection)\n",
    "obs_points = geemap.df_to_ee(obs_geom, \n",
    "                             latitude='Latitude (decimal degrees)', \n",
    "                             longitude='Longitude (decimal degrees)')\n",
    "\n",
    "# select points that intercept HUC\n",
    "samplepoints = obs_points.filterBounds(grid_location_ee)\n",
    "\n",
    "# Create dictionary of grid coordinates\n",
    "points_dict = samplepoints.getInfo()\n",
    "feats = points_dict['features']\n",
    "\n",
    "# Create a list of several ee.Geometry.Polygons\n",
    "points = []\n",
    "for f in feats:\n",
    "    coords = f['geometry']['coordinates']\n",
    "    point = ee.Geometry.Point(coords)\n",
    "    # create buffer around point for later reduce regions\n",
    "    buffered = point.buffer(POINTBUFFER)\n",
    "    points.append(buffered)\n",
    "\n",
    "# Make a feature collection for export purposes\n",
    "points_ee = ee.FeatureCollection(points)\n",
    "print(f'{len(points)} {POINTBUFFER}-meter buffered points within HUC6 {CURRENTROI}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "releve_nums = []\n",
    "for f in feats:\n",
    "    id = f['properties']['Releve number']\n",
    "    releve_nums.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(releve_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626126f",
   "metadata": {},
   "source": [
    "# 5. S2-SR Cloud filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(\n",
    "        primary = s2_sr_col,\n",
    "        secondary = s2_cloudless_col,\n",
    "        condition = ee.Filter.equals(\n",
    "            leftField = 'system:index',\n",
    "            rightField = 'system:index')\n",
    "    ))\n",
    "\n",
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "\n",
    "\n",
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')))\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "\n",
    "\n",
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)\n",
    "\n",
    "\n",
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    #return img.select('B*').updateMask(not_cld_shdw)\n",
    "    return img.updateMask(not_cld_shdw).select(BANDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc1e93",
   "metadata": {},
   "source": [
    "# 6. NDVI and date band creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd89a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeField = 'system:time_start'\n",
    "def add_variables(image):\n",
    "    # Compute time in fractional years since the epoch.\n",
    "    date = ee.Date(image.get(timeField)).millis()\n",
    "    # Return the image with the added bands.\n",
    "    return (image\n",
    "            .addBands(image.normalizedDifference(['B8', 'B4']).rename('ndvi'))\n",
    "            .addBands(ee.Image(date).rename('date').float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587ba30",
   "metadata": {},
   "source": [
    "# 7. Apply cloud mask, NDVI, and date functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_sr_cld_col = get_s2_sr_cld_col(samplepoints, str(start_date), str(end_date))\n",
    "s2_sr = (s2_sr_cld_col\n",
    "         .map(add_cld_shdw_mask)\n",
    "         .map(apply_cld_shdw_mask)\n",
    "         .map(add_variables)).select(['ndvi', 'date'])\n",
    "\n",
    "print(\"Shadow mask applied to tiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f149615",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf85b2c",
   "metadata": {},
   "source": [
    "# 8. Create median composite for test time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a42be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through date ranges and export sampled composites\n",
    "#for RANGE in date_ranges:\n",
    "RANGE = date_ranges[14]\n",
    "    \n",
    "# select cloud-filtered sentinel 2 imagery for time step\n",
    "s2_by_date = s2_sr.filterDate(RANGE[0], RANGE[1])\n",
    "\n",
    "#for i, point in enumerate(points):\n",
    "\n",
    "sentinel2 = s2_by_date.filterBounds(points_ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage composites with no data\n",
    "if sentinel2.size().getInfo() != 0:\n",
    "    # create composite for time step\n",
    "    composite = (sentinel2.select('ndvi')).median()\n",
    "    composite_date = (sentinel2.select('date')).reduce(ee.Reducer.first())\n",
    "    composite = composite.addBands(composite_date)\n",
    "else:\n",
    "    composite = ee.Image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f820c",
   "metadata": {},
   "source": [
    "# 9. Sample composites at points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample composite with points (returns feature collection)\n",
    "sampled = composite.reduceRegions(collection = points_ee,\n",
    "                                  reducer = ee.Reducer.median(),\n",
    "                                  scale = SCALE,\n",
    "                                  crs = 'EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca0317",
   "metadata": {},
   "source": [
    "# 10. Export ndvi and date sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geemap.ee_to_pandas(sampled, col_names=['date_first', 'ndvi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e47630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export feature collection as csv\n",
    "df.to_csv(f'{DIR_PATH}/{RANGE[0]}_to_{RANGE[1]}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
